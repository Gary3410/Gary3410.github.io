<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation">
    <meta name="author" content="Zhenyu Wu,
                                 Yuheng Zhou,
                                 Xiuwei Xu,
                                 Ziwei Wang,
                                 Haibin Yan">

    <title>MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation</h2>
    <h3>CVPR 2025</h3>
<!--            <p class="abstract">An interpretable, data-efficient, and scalable neural scene representation.</p>-->
    <hr>
    <p>
        <span style="white-space: nowrap; font-size:larger">
            <a href="https://gary3410.github.io/">Zhenyu Wu</a><sup>1</sup>&nbsp;&nbsp;
            Yuheng Zhou</a><sup>2</sup>&nbsp;&nbsp;
            <a href="https://xuxw98.github.io/">Xiuwei Xu</a><sup>3</sup>&nbsp;&nbsp;
            <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a><sup>2</sup>&nbsp;&nbsp;
            Haibin Yan<sup>1&#8224;</sup>
        </span>
        <br><br>
        <sup>1</sup>Beijing University of Posts and Telecommunications&nbsp;&nbsp;<sup>2</sup>Nanyang Technological University&nbsp;&nbsp;<sup>3</sup>Tsinghua University<br>
        <br><br>
        <a href="http://arxiv.org/abs/2503.13446" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/material-outlined/24/000000/file.png" alt="paper" style="vertical-align: middle;">
            &nbsp;Paper (arXiv)
        </a>&nbsp;&nbsp;&nbsp;&nbsp;
        <a href="https://gary3410.github.io/momanipVLA/" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/material-outlined/24/000000/github.png" alt="code" style="vertical-align: middle;">
            &nbsp;Code (Coming Soon)
        </a>
<!--          <a href="https://gary3410.github.io/eif_unknown/" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/ios-filled/50/database-options.png" width="60%" alt="code" style="vertical-align: middle;">
            &nbsp;Dataset (Coming Soon)
        </a> -->
    </p>

    <!-- <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2006.09661">Paper</a>
        <a class="btn btn-primary" href="https://colab.research.google.com/github/vsitzmann/siren/blob/master/explore_siren.ipynb">Colab Notebook</a>
        <a class="btn btn-primary" href="https://dcato98.github.io/playground/#activation=sine">Tensorflow Playground</a>
        <a class="btn btn-primary" href="https://github.com/vsitzmann/siren">Code</a>
        <a class="btn btn-primary" href="https://drive.google.com/drive/u/1/folders/1_iq__37-hw7FJOEUK1tX7mdp8SKB368K">Data</a>
    </div> -->
</div>

<div class="container">
    <div class="vcontainer">
        <iframe class='video' src="https://www.youtube.com/embed/9C9q9TMZyfY" frameborder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
    </div>
     <div class="vcontainer">
            <iframe class="video" src="https://www.youtube.com/embed/5_ysVBoQL0Y" frameborder="0"
                    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen></iframe>
        </div>

    <div class="section">
            <h2>Abstract</h2>
            <hr>
            <p>
                <hr>
            <p>
                Mobile manipulation is the fundamental challenge for robotics to assist humans with diverse tasks and environments in everyday life. However, conventional mobile manipulation approaches often struggle to generalize across different tasks and environments because of the lack of large-scale training. In contrast, recent advances in vision-language-action (VLA) models have shown impressive generalization capabilities, but these foundation models are developed for fixed-base manipulation tasks.
            </p>
            <p>
                Therefore, we propose an efficient policy adaptation framework named <b>MoManipVLA</b> to transfer pre-trained VLA models of fixed-base manipulation to mobile manipulation, so that high generalization ability across tasks and environments can be achieved in mobile manipulation policy. Specifically, we utilize pre-trained VLA models to generate waypoints of the end-effector with high generalization ability. We design motion planning objectives for the mobile base and the robot arm, which aim at maximizing the physical feasibility of the trajectory.
            </p>
            <p>
                Finally, we present an efficient bi-level objective optimization framework for trajectory generation, where the upper-level optimization predicts waypoints for base movement to enhance the manipulator policy space, and the lower-level optimization selects the optimal end-effector trajectory to complete the manipulation task. In this way, <b>MoManipVLA</b> can adjust the position of the robot base in a zero-shot manner, thus making the waypoints predicted from the fixed-base VLA models feasible.
            </p>
            <p>
                Extensive experimental results on OVMM and the real world demonstrate that <b>MoManipVLA</b> achieves a <b>4.2% higher success rate</b> than the state-of-the-art mobile manipulation, and only requires <b>50% training cost</b> for real-world deployment due to the strong generalization ability in the pre-trained VLA models.
            </p>
            </p>
            <div class="row align-items-center">
                <div class="col text-center">
                    <img src="img/Figure1_v2.png" alt="pipeline" width="100%">
                </div>
            </div>
        </div>

    <div class="section">
        <h2>Approach</h2>
        <hr>
        <p>
            <b>The pipeline of MoManipVLA.</b> The pre-trained VLA models predict highly generalized end-effector waypoints to guide the mobile manipulation task, through which the trajectory of the mobile base and the robot arm can be generated with objectives of physical feasibility. The objectives consider the reachability, smoothness, and collision, and the trajectory is acquired via bi-level optimization.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/Figure2_v2.png" alt="pipeline" width="100%">
            </div>
        </div>
    </div>
    

    <hr>
</div>

    <p><center>
        <div id="clustrmaps-widget" style="width:30%">
            <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=bTSyNuGn38AoOOhwBS1NL_UJF26NbR3bhmOet92JUcE&cl=ffffff&w=a"></script>
        </div>
        <br>
        &copy; Zhenyu Wu | Last update: June. 17, 2024
    </center></p>

<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>

<!--        @article{xu2024online,-->
<!--          title={Memory-based Adapters for Online 3D Scene Perception},-->
<!--          author={Xiuwei Xu and Chong Xia and Ziwei Wang and Linqing Zhao and Yueqi Duan and Jie Zhou and Jiwen Lu},-->
<!--          journal={arXiv preprint arXiv:2403.06974},-->
<!--          year={2024}-->
<!--        }-->
