<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Embodied Instruction Following in Unknown Environments">
    <meta name="author" content="Zhenyu Wu,
                                 Ziwei Wang,
                                 Xiuwei Xu,
                                 Jiwen Lu,
                                 Haibin Yan">

    <title>Embodied Instruction Following in Unknown Environments</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>Embodied Instruction Following in Unknown Environments</h2>
    <h3>arXiv 2024</h3>
<!--            <p class="abstract">An interpretable, data-efficient, and scalable neural scene representation.</p>-->
    <hr>
    <p>
        <span style="white-space: nowrap; font-size:larger">
            <a href="https://gary3410.github.io/">Zhenyu Wu</a><sup>1</sup>&nbsp;&nbsp;
            <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a><sup>2</sup>&nbsp;&nbsp;
            <a href="https://xuxw98.github.io/">Xiuwei Xu</a><sup>3</sup>&nbsp;&nbsp;
            <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a><sup>3</sup>&nbsp;&nbsp;
            Haibin Yan<sup>1&#8224;</sup>
        </span>
        <br><br>
        <sup>1</sup>Beijing University of Posts and Telecommunications&nbsp;&nbsp;<sup>2</sup>Carnegie Mellon University&nbsp;&nbsp;<sup>3</sup>Tsinghua University<br>
        <br><br>
        <a href="https://gary3410.github.io/" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/material-outlined/24/000000/file.png" alt="paper" style="vertical-align: middle;">
            &nbsp;Paper (arXiv)
        </a>&nbsp;&nbsp;&nbsp;&nbsp;
        <a href="https://gary3410.github.io/eif_unknown/" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/material-outlined/24/000000/github.png" alt="code" style="vertical-align: middle;">
            &nbsp;Code (Coming Soon)
        </a>
         <a href="https://gary3410.github.io/eif_unknown/" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/ios-filled/50/database-options.png" alt="code" style="vertical-align: middle;">
            &nbsp;Dataset (Coming Soon)
        </a>
    </p>

    <!-- <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2006.09661">Paper</a>
        <a class="btn btn-primary" href="https://colab.research.google.com/github/vsitzmann/siren/blob/master/explore_siren.ipynb">Colab Notebook</a>
        <a class="btn btn-primary" href="https://dcato98.github.io/playground/#activation=sine">Tensorflow Playground</a>
        <a class="btn btn-primary" href="https://github.com/vsitzmann/siren">Code</a>
        <a class="btn btn-primary" href="https://drive.google.com/drive/u/1/folders/1_iq__37-hw7FJOEUK1tX7mdp8SKB368K">Data</a>
    </div> -->
</div>

<div class="container">
    <div class="vcontainer">
        <iframe class='video' src="https://www.youtube.com/embed/4cwEmzw-J7E?si=xwRZ58mVJmeB_1HE" frameborder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
    </div>

    <div class="section">
        <h2>Abstract</h2>
        <hr>
        <p>
            Enabling embodied agents to complete complex human instructions from natural language is crucial to autonomous systems in household services. Conventional methods can only accomplish human instructions in the known environment where all interactive objects are provided to the embodied agent, and directly deploying the existing approaches for the unknown environment usually generates infeasible plans that manipulate non-existing objects. On the contrary, we propose an embodied instruction following (EIF) method for complex tasks in the unknown environment, where the agent efficiently explores the unknown environment to generate feasible plans with existing objects to accomplish abstract instructions. Specifically, we build a hierarchical embodied instruction following framework including the high-level task planner and the low-level exploration controller with multimodal large language models. We then construct a semantic representation map of the scene with dynamic region attention to demonstrate the known visual clues, where the goal of task planning and scene exploration is aligned for human instruction. For the task planner, we generate the feasible step-by-step plans for human goal accomplishment according to the task completion process and the known visual clues. For the exploration controller, the optimal navigation or object interaction policy is predicted based on the generated step-wise plans and the known visual clues. The experimental results demonstrate that our method can achieve 45.09% success rate in 204 complex human instructions such as making breakfast and tidying rooms in large house-level scenes.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/Figure_1_v5.png" alt="pipeline" width="100%">
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Approach</h2>
        <hr>
        <p>
            <b>Overview of our approach.</b> The scene feature map is constructed based on real-time RGB-D images, which is leveraged as visual clues for the high-level planner and the low-level controller. The planner generates the step-wise plans, which are leveraged to predict the specific actions in the controller. The optimal border between unknown and known regions is selected for scene exploration, and the scene feature map is updated with the visual clues seen in during the exploration.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/Figure_2_v5.png" alt="pipeline" width="100%">
            </div>
        </div>
    </div>

    <div class="section">
        <h2 style="color: #FF5733;">Highlights</h2>
        <hr>

        <h2 class="title is-4">Long-sequence Task Planning</h2>
        <div class="vcontainer">
        <iframe class='video' src="https://www.youtube.com/embed/p3yP6wOAwj8?si=H7BquTttqaUEG8Mr" frameborder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
        </div>
        <p>
            We demonstrate that the proposed hierarchical planning framework enables robots to have the ability to perform long sequential tasks (e.g., operating a microwave oven). By generating step-by-step planning, the robot is able to understand the execution progress based on contextual information.
        </p>
        <h2 class="title is-4">Active Interactive Exploration</h2>
        <div class="vcontainer">
        <iframe class='video' src="https://www.youtube.com/embed/wPU2w9sSSPU?si=34b8HRoTSTEl-2bD" frameborder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
        </div>
        <p>
            In unknown environments, our framework enables robots to interact with the environment to explore more visual information actively. The robot generates the exploration action of <b> opening a fridge to look for eggs </b> instead of navigating randomly.
        </p>
    </div>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
            <div class="bibtexsection">
        @article{wu2023embodied,
          title={Embodied task planning with large language models},
          author={Wu, Zhenyu and Wang, Ziwei and Xu, Xiuwei and Lu, Jiwen and Yan, Haibin},
          journal={arXiv preprint arXiv:2307.01848},
          year={2023}
        }
            </div>
        </div>
    </div>

    <hr>
</div>

    <p><center>
        <div id="clustrmaps-widget" style="width:30%">
            <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=dyEJK0u2r6a9O69xqc1-SkjNbEVEc2ikYsdLK5y9Iis&cl=ffffff&w=a"></script>
        </div>
        <br>
        &copy; Zhenyu Wu | Last update: June. 17, 2024
    </center></p>

<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>

<!--        @article{xu2024online,-->
<!--          title={Memory-based Adapters for Online 3D Scene Perception},-->
<!--          author={Xiuwei Xu and Chong Xia and Ziwei Wang and Linqing Zhao and Yueqi Duan and Jie Zhou and Jiwen Lu},-->
<!--          journal={arXiv preprint arXiv:2403.06974},-->
<!--          year={2024}-->
<!--        }-->
